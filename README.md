

# ğŸŒ Multi-Lingual Sentiment Analysis

**Fine-tuning LLaMA 3.1-8B-Instruct using LoRA (NPPE-1 Competition)**

## ğŸ“Œ Project Overview

This repository contains my solution for the **NPPE-1 Multilingual Sentiment Analysis Competition**, where the objective was to fine-tune **LLaMA 3.1-8B-Instruct** for **sentiment classification across 13 Indian languages** under strict compute constraints.

Although LLaMA 3.1 officially supports only Hindi, its tokenizer and pretraining corpus include all target languages. This project demonstrates how **parameter-efficient fine-tuning (LoRA)** enables effective multilingual adaptation using **limited labeled data** and **Kaggle / Colab-level compute**.

---

## ğŸ¯ Problem Statement

* Perform sentiment classification on text written in **13 Indian languages**
* Work with **limited labeled data**
* Use **Kaggle Notebooks / Colab only** (no external GPUs)
* Fine-tune a large language model efficiently without full retraining

---

## ğŸ§  Key Ideas Explored

* Multilingual adaptation of instruction-tuned LLMs
* Low-resource and data-efficient learning
* Parameter-Efficient Fine-Tuning (PEFT) using **LoRA**
* Prompt-based sentiment classification
* Trade-offs between performance and compute constraints

---

## âš™ï¸ Model & Training Details

### ğŸ”¹ Base Model

* **LLaMA 3.1-8B-Instruct**

### ğŸ”¹ Fine-Tuning Method

* **LoRA (Low-Rank Adaptation)**
* Applied to attention layers to reduce trainable parameters
* Keeps memory usage low while enabling task adaptation

### ğŸ”¹ Training Environment

* Platform: **Kaggle Notebook / Google Colab**
* Mixed-precision training
* Optimized batch size and gradient accumulation to fit memory limits

### ğŸ”¹ Task Formulation

* Sentiment classification framed as an **instruction-following task**
* Unified label space across all languages
* Language-agnostic prompting to encourage cross-lingual generalization

---


## ğŸ“Š Evaluation

* Metric: **Classification Accuracy**
* Evaluated across all languages
* Focus on **generalization**, not just high-resource languages

---

## ğŸ“ Repository Contents

```text
â”œâ”€â”€ llama3-1-tuned-for-sentiment-classification.ipynb
â”‚   â””â”€â”€ Colab notebook containing preprocessing, LoRA fine-tuning, and evaluation
â”‚
â”œâ”€â”€ multi-lingual-sentiment-analysis.zip
â”‚   â””â”€â”€ Data used for training and testing
â”‚
â””â”€â”€ README.md
```

---

## ğŸš€ Key Outcomes

* Successfully adapted LLaMA 3.1 for multilingual sentiment analysis
* Demonstrated effectiveness of **LoRA under compute constraints**
* Achieved robust performance across multiple low-resource languages
* Gained hands-on experience with **scalable LLM fine-tuning pipelines**

---

## ğŸ› ï¸ Tech Stack

* Python
* PyTorch
* Hugging Face Transformers
* PEFT (LoRA)
* Kaggle / Google Colab

---


